{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "os.environ[\"OMP_NUM_THREADS\"]= \"4\"\n",
    "\n",
    "import argparse\n",
    "import yaml\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from models.full_model import ModelAGDsup as Model\n",
    "from dataset.data import get_loader as get_loader\n",
    "from models.metric import KLD, SIM, NSS\n",
    "\n",
    "\n",
    "def set_random_seed(seed, deterministic=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        INTERPOLATE_MODE = \"nearest\"\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    else:\n",
    "        INTERPOLATE_MODE = \"bilinear\"\n",
    "    return INTERPOLATE_MODE\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Finetuning on AGD20K')\n",
    "    parser.add_argument('--config', type=str, help='Path to the configuration file', required=True)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    " \n",
    "  \n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return config\n",
    "\n",
    "\n",
    "def plot_annotation(image, heatmap, alpha=0.5, name=\"\"):\n",
    "    \"\"\"Plot the heatmap on the target image.\n",
    "\n",
    "    Args:\n",
    "        image: The target image.\n",
    "        points: The annotated points.\n",
    "        heatmap: The generated heatmap.\n",
    "        alpha: The alpha value of the overlay image.\n",
    "    \"\"\"\n",
    "    # Plot the overlay of heatmap on the target image.\n",
    "    processed_heatmap = heatmap * 255 / np.max(heatmap)\n",
    "    processed_heatmap = np.tile(processed_heatmap[:, :, np.newaxis], (1, 1, 3)).squeeze(2)\n",
    "    processed_heatmap = processed_heatmap.astype('uint8')\n",
    "    processed_heatmap = cv2.applyColorMap(processed_heatmap, cv2.COLORMAP_JET)\n",
    "    # print(processed_heatmap.shape, image.shape)\n",
    "    # assert processed_heatmap.shape == image.shape\n",
    "    overlay = cv2.addWeighted(processed_heatmap, alpha, image, 1-alpha, 0) # TODO: [:, :, ::-1]\n",
    "    # cv2.imwrite(name, overlay) # TODO: , cv2.COLOR_BGR2RGB)\n",
    "    cv2.show(overlay) # TODO: , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            \n",
    "def plot_annotation_with_gt(image, heatmap, gt, alpha=0.5, name=\"\"):\n",
    "    \"\"\"Plot the heatmap on the target image.\n",
    "\n",
    "    Args:\n",
    "        image: The target image.\n",
    "        points: The annotated points.\n",
    "        heatmap: The generated heatmap.\n",
    "        gt: The ground truth mask.\n",
    "        alpha: The alpha value of the overlay image.\n",
    "    \"\"\"\n",
    "    # Plot the overlay of heatmap on the target image.\n",
    "    processed_heatmap = heatmap * 255 / np.max(heatmap)\n",
    "    processed_heatmap = np.tile(processed_heatmap[:, :, np.newaxis], (1, 1, 3)).squeeze(2)\n",
    "    processed_heatmap = processed_heatmap.astype('uint8')\n",
    "    processed_heatmap = cv2.applyColorMap(processed_heatmap, cv2.COLORMAP_JET)\n",
    "    # print(processed_heatmap.shape, image.shape)\n",
    "    # assert processed_heatmap.shape == image.shape\n",
    "    print(\"shape:\",processed_heatmap.shape, image.shape)\n",
    "    overlay = cv2.addWeighted(processed_heatmap, alpha, image, 1-alpha, 0) # TODO: [:, :, ::-1]\n",
    "    \n",
    "    ### Process the ground truth mask\n",
    "    # Plot the overlay of heatmap on the target image.\n",
    "    processed_gt = gt * 255 / np.max(gt)\n",
    "    processed_gt = np.tile(processed_gt[:, :, np.newaxis], (1, 1, 3)).squeeze(2)\n",
    "    processed_gt = processed_gt.astype('uint8')\n",
    "    processed_gt = cv2.applyColorMap(processed_gt, cv2.COLORMAP_JET)\n",
    "    \n",
    "    overlay_gt = cv2.addWeighted(processed_gt, alpha, image, 1-alpha, 0) # TODO: [:, :, ::-1]\n",
    "\n",
    "    concat = np.concatenate([overlay, overlay_gt], axis=1)\n",
    "    # cv2.imwrite(name, concat)\n",
    "    cv2.imshow(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir:  logs/seen_test\n",
      "PL_mode: refined\n",
      "aug4imgRatio: 0.5\n",
      "batch_size: 10\n",
      "data_dir: ../../AGD20K\n",
      "deterministic: false\n",
      "img_size: 224\n",
      "load:\n",
      "  all_ckpt: ./logs/seen/ckpt/bestKLD.ckpt\n",
      "  encoder_ckpt: null\n",
      "model:\n",
      "  decoder_embed_dim: 512\n",
      "  encoder_params:\n",
      "    heads: 12\n",
      "    layers: 12\n",
      "    output_dim: 512\n",
      "    width: 768\n",
      "  encoder_type: CLIP\n",
      "  margin: 0.1\n",
      "  pred_decoder_args:\n",
      "    conv_first: true\n",
      "    depth: 2\n",
      "    mlp_dim: 2048\n",
      "    use_additional_token: true\n",
      "    use_up: 2\n",
      "  pred_model_type: SAM\n",
      "num_exo: 1\n",
      "split_type: Seen\n",
      "work_dir: logs/seen_test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_pth=\"configs/seen_test.yaml\"\n",
    "config = load_config(cfg_pth)\n",
    "\n",
    "os.makedirs(f\"{config['work_dir']}\", exist_ok=True)\n",
    "print(\"save_dir: \", config['work_dir'])\n",
    "\n",
    "if not os.path.exists(f\"{config['work_dir']}/ckpt\"):\n",
    "    os.makedirs(f\"{config['work_dir']}/ckpt\")\n",
    "if not os.path.exists(f\"{config['work_dir']}/img\"):\n",
    "    os.makedirs(f\"{config['work_dir']}/img\")\n",
    "\n",
    "args_text = yaml.safe_dump(config, default_flow_style=False)\n",
    "print(args_text)\n",
    "\n",
    "INTERPOLATE_MODE = set_random_seed(1, deterministic=config[\"deterministic\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlee/.conda/envs/wsag-plsp/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_config = config['model']\n",
    "model = Model(**model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config = config[\"load\"]\n",
    "all_ckpt, encoder_ckpt = load_config[\"all_ckpt\"], load_config[\"encoder_ckpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/seen/ckpt/bestKLD.ckpt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> start_load\n",
      ">> loaded\n",
      "Loaded from  ./logs/seen/ckpt/bestKLD.ckpt\n",
      "#Params: 112520740\n",
      "#Encoder Params: 86192640\n",
      "#Final Decoder Params: 12152576\n"
     ]
    }
   ],
   "source": [
    "### N분정도 소요됨 (6분 이하..?)\n",
    "load_config = config[\"load\"]\n",
    "all_ckpt, encoder_ckpt = load_config[\"all_ckpt\"], load_config[\"encoder_ckpt\"]\n",
    "if all_ckpt:\n",
    "    with open(\"/home/shlee/workspace/02_robotics_miniProject/4_Affordance/WSAG-PLSP/codes/logs/seen/ckpt/bestKLD.ckpt\", \"rb\") as f:\n",
    "        print(\">> start_load\")\n",
    "        state_dict = torch.load(f)[\"state_dict\"]\n",
    "        print(\">> loaded\")\n",
    "    print(\"Loaded from \", all_ckpt)\n",
    "    u, w = model.load_state_dict(state_dict, False)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "num_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_parameters}')\n",
    "num_parameters = sum([p.numel() for p in model.encoder.parameters()])\n",
    "print(f'#Encoder Params: {num_parameters}')\n",
    "num_parameters = sum([p.numel() for p in model.pred_decoder.parameters()])\n",
    "print(f'#Final Decoder Params: {num_parameters}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model).cuda()\n",
    "    \n",
    "model.eval()\n",
    "vall_kld = 0.\n",
    "vall_sim = 0.\n",
    "vall_nss = 0.\n",
    "vall_num = 0\n",
    "vall_num_sum = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f20ba28d310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data_loader = get_loader(\n",
    "        batch_size=1,\n",
    "        img_size=config[\"img_size\"], # follow LOCATE, Cross-View-AG, eval at 224*224\n",
    "        split_file=config[\"split_type\"],\n",
    "        data_dir=config[\"data_dir\"],\n",
    "        shuffle=False,\n",
    "        train=False,\n",
    "        exo_obj_file=None, \n",
    "        ego_obj_file=None, \n",
    "        no_pad_gt=True\n",
    "    )\n",
    "eval_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../AGD20K'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beat': 0,\n",
       " 'boxing': 1,\n",
       " 'brush_with': 2,\n",
       " 'carry': 3,\n",
       " 'catch': 4,\n",
       " 'cut': 5,\n",
       " 'cut_with': 6,\n",
       " 'drag': 7,\n",
       " 'drink_with': 8,\n",
       " 'eat': 9,\n",
       " 'hit': 10,\n",
       " 'hold': 11,\n",
       " 'jump': 12,\n",
       " 'kick': 13,\n",
       " 'lie_on': 14,\n",
       " 'lift': 15,\n",
       " 'look_out': 16,\n",
       " 'open': 17,\n",
       " 'pack': 18,\n",
       " 'peel': 19,\n",
       " 'pick_up': 20,\n",
       " 'pour': 21,\n",
       " 'push': 22,\n",
       " 'ride': 23,\n",
       " 'sip': 24,\n",
       " 'sit_on': 25,\n",
       " 'stick': 26,\n",
       " 'stir': 27,\n",
       " 'swing': 28,\n",
       " 'take_photo': 29,\n",
       " 'talk_on': 30,\n",
       " 'text_on': 31,\n",
       " 'throw': 32,\n",
       " 'type_on': 33,\n",
       " 'wash': 34,\n",
       " 'write': 35}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data_loader.dataset.verb2vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drum': 0,\n",
       " 'punching_bag': 1,\n",
       " 'toothbrush': 2,\n",
       " 'skateboard': 3,\n",
       " 'skis': 4,\n",
       " 'snowboard': 5,\n",
       " 'surfboard': 6,\n",
       " 'frisbee': 7,\n",
       " 'rugby_ball': 8,\n",
       " 'soccer_ball': 9,\n",
       " 'apple': 10,\n",
       " 'banana': 11,\n",
       " 'carrot': 12,\n",
       " 'orange': 13,\n",
       " 'knife': 14,\n",
       " 'scissors': 15,\n",
       " 'suitcase': 16,\n",
       " 'bottle': 17,\n",
       " 'cup': 18,\n",
       " 'wine_glass': 19,\n",
       " 'broccoli': 20,\n",
       " 'hot_dog': 21,\n",
       " 'axe': 22,\n",
       " 'baseball_bat': 23,\n",
       " 'hammer': 24,\n",
       " 'tennis_racket': 25,\n",
       " 'badminton_racket': 26,\n",
       " 'book': 27,\n",
       " 'bowl': 28,\n",
       " 'fork': 29,\n",
       " 'golf_clubs': 30,\n",
       " 'bed': 31,\n",
       " 'bench': 32,\n",
       " 'couch': 33,\n",
       " 'binoculars': 34,\n",
       " 'microwave': 35,\n",
       " 'oven': 36,\n",
       " 'refrigerator': 37,\n",
       " 'bicycle': 38,\n",
       " 'motorcycle': 39,\n",
       " 'chair': 40,\n",
       " 'camera': 41,\n",
       " 'cell_phone': 42,\n",
       " 'baseball': 43,\n",
       " 'basketball': 44,\n",
       " 'discus': 45,\n",
       " 'javelin': 46,\n",
       " 'keyboard': 47,\n",
       " 'laptop': 48,\n",
       " 'pen': 49}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data_loader.dataset.noun2nid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['drum', 'baseball', 'rugby_ball', 'soccer_ball', 'javelin', 'frisbee', 'discus', 'basketball', 'cup', 'bottle', 'wine_glass', 'punching_bag', 'camera', 'cell_phone', 'suitcase', 'skis', 'laptop', 'keyboard', 'refrigerator', 'microwave', 'book', 'oven', 'bowl', 'pen', 'bicycle', 'motorcycle', 'tennis_racket', 'golf_clubs', 'badminton_racket', 'baseball_bat', 'surfboard', 'skateboard', 'snowboard', 'fork', 'knife', 'toothbrush', 'orange', 'apple', 'carrot', 'banana', 'bench', 'couch', 'bed', 'chair', 'scissors', 'hammer', 'axe', 'broccoli', 'hot_dog', 'binoculars'])\n",
      "dict_keys(['beat', 'throw', 'sip', 'kick', 'pour', 'take_photo', 'drink_with', 'pick_up', 'type_on', 'open', 'stir', 'talk_on', 'catch', 'text_on', 'write', 'ride', 'push', 'swing', 'drag', 'boxing', 'jump', 'wash', 'peel', 'cut', 'lie_on', 'sit_on', 'brush_with', 'cut_with', 'pack', 'hit', 'carry', 'hold', 'eat', 'look_out', 'lift', 'stick'])\n",
      "dict_keys(['beat drum', 'throw baseball', 'throw rugby_ball', 'throw soccer_ball', 'throw javelin', 'throw frisbee', 'throw discus', 'throw basketball', 'sip cup', 'sip bottle', 'sip wine_glass', 'kick rugby_ball', 'kick soccer_ball', 'kick punching_bag', 'pour cup', 'pour bottle', 'pour wine_glass', 'take_photo camera', 'take_photo cell_phone', 'drink_with cup', 'drink_with bottle', 'drink_with wine_glass', 'pick_up suitcase', 'pick_up skis', 'type_on laptop', 'type_on keyboard', 'open suitcase', 'open bottle', 'open refrigerator', 'open microwave', 'open book', 'open oven', 'stir bowl', 'talk_on cell_phone', 'catch rugby_ball', 'catch soccer_ball', 'catch frisbee', 'text_on cell_phone', 'write pen', 'ride bicycle', 'ride motorcycle', 'push bicycle', 'push motorcycle', 'swing tennis_racket', 'swing golf_clubs', 'swing badminton_racket', 'swing baseball_bat', 'drag suitcase', 'boxing punching_bag', 'jump skis', 'jump surfboard', 'jump skateboard', 'jump snowboard', 'wash fork', 'wash cup', 'wash knife', 'wash toothbrush', 'wash bowl', 'wash wine_glass', 'wash orange', 'peel apple', 'peel orange', 'peel carrot', 'peel banana', 'cut apple', 'cut orange', 'cut carrot', 'cut banana', 'lie_on bench', 'lie_on couch', 'lie_on bed', 'lie_on surfboard', 'sit_on bench', 'sit_on couch', 'sit_on bed', 'sit_on chair', 'sit_on surfboard', 'sit_on bicycle', 'sit_on skateboard', 'sit_on motorcycle', 'brush_with toothbrush', 'cut_with knife', 'cut_with scissors', 'pack suitcase', 'hit hammer', 'hit tennis_racket', 'hit axe', 'hit baseball_bat', 'carry skis', 'carry surfboard', 'carry skateboard', 'carry snowboard', 'hold suitcase', 'hold hammer', 'hold fork', 'hold cup', 'hold bottle', 'hold skis', 'hold knife', 'hold frisbee', 'hold toothbrush', 'hold bowl', 'hold tennis_racket', 'hold axe', 'hold golf_clubs', 'hold wine_glass', 'hold surfboard', 'hold book', 'hold skateboard', 'hold snowboard', 'hold badminton_racket', 'hold baseball_bat', 'hold scissors', 'eat broccoli', 'eat apple', 'eat hot_dog', 'eat orange', 'eat carrot', 'eat banana', 'look_out binoculars', 'lift fork', 'stick fork', 'stick knife', 'wash carrot'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check text feature\n",
    "\n",
    "# data_dir=\"../../AGD20K\"\n",
    "# nounsFeat = torch.load(os.path.join(data_dir, \"sentenceFeatNounAGD.pth\"))\n",
    "# verbsFeat = torch.load(os.path.join(data_dir, \"sentenceFeatVerbAGD.pth\"))\n",
    "# partsFeat = torch.load(os.path.join(data_dir, \"sentenceFeatPartAGD.pth\"))\n",
    "# print(nounsFeat.keys())\n",
    "# print(verbsFeat.keys())\n",
    "# print(partsFeat.keys())\n",
    "# verbsFeat[\"beat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partsFeat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9030e-03,  4.7852e-02,  1.4282e-01, -3.5217e-02, -4.0112e-01,\n",
       "         1.9913e-02, -2.3828e-01,  3.1219e-02, -4.6204e-02,  5.0385e-02,\n",
       "        -1.5762e-02,  1.7297e-01, -8.8989e-02,  4.6661e-02,  1.7908e-01,\n",
       "        -1.0382e-01, -1.2134e-01, -8.0713e-01,  2.7664e-02, -1.9873e-01,\n",
       "        -1.4429e-01,  2.2583e-01, -7.9773e-02,  7.0923e-02,  8.2245e-03,\n",
       "         3.0838e-02,  1.1139e-01,  2.7490e-01, -2.0850e-01, -7.3181e-02,\n",
       "         1.7065e-01,  3.8300e-02, -2.0142e-01, -1.4124e-01,  4.6289e-01,\n",
       "         4.3384e-01, -1.1462e-01, -4.3896e-01,  4.3628e-01,  1.2006e-01,\n",
       "        -2.1118e-01, -1.0907e-01, -1.1047e-01,  3.6678e-03,  2.1582e-01,\n",
       "        -1.7053e-01, -3.8501e-01,  2.0190e-01,  3.2318e-02, -7.8003e-02,\n",
       "         1.7322e-01,  3.2080e-01, -1.3451e-02, -3.2422e-01, -1.0345e-01,\n",
       "         1.6602e-01,  5.9521e-01,  4.6875e-02,  5.2551e-02,  2.0386e-01,\n",
       "         6.9702e-02, -9.8206e-02, -1.6577e-01,  3.0859e-01,  8.8501e-04,\n",
       "         2.9510e-02,  5.4962e-02,  2.1118e-01,  2.7344e-01, -2.8369e-01,\n",
       "         2.4719e-02,  2.2351e-01,  1.4868e-01, -1.4795e-01,  4.0186e-01,\n",
       "        -1.8555e-01, -7.0374e-02,  7.3242e-03,  1.0022e-01, -6.3416e-02,\n",
       "         1.4673e-01,  8.3643e-01, -1.4343e-01,  1.2802e-02, -4.6082e-02,\n",
       "         4.3243e-02, -1.2866e-01, -1.2500e-01, -9.4482e-02, -4.6021e-02,\n",
       "         3.7567e-02,  2.7563e-01,  7.0251e-02, -6.5186e-02,  2.4561e-01,\n",
       "        -3.9380e-01, -2.4355e+00, -1.4014e-01, -2.5293e-01, -2.6514e-01,\n",
       "         6.1073e-03, -1.1389e-01,  1.2573e-01,  2.7783e-01,  7.6355e-02,\n",
       "         2.0096e-02, -3.6682e-02,  1.5955e-01,  1.7932e-01,  3.9139e-03,\n",
       "        -6.1554e-02,  2.1008e-01,  5.1636e-02, -8.9798e-03, -2.8397e-02,\n",
       "         2.0471e-01,  9.2468e-02, -1.3329e-02,  1.4023e-02,  2.9956e-01,\n",
       "        -7.7576e-02, -2.2568e-02,  8.9111e-02, -3.5645e-02,  1.5442e-01,\n",
       "         9.8938e-02,  4.8553e-02,  2.8760e-01,  1.9482e-01,  1.6895e-01,\n",
       "         2.5070e-02,  2.5589e-02,  1.1768e-01,  3.1082e-02, -7.7515e-02,\n",
       "         1.1456e-01, -6.8542e-02, -2.5317e-01,  1.2489e-02,  1.0419e-01,\n",
       "        -2.3608e-01,  2.9517e-01,  2.3071e-01,  5.2307e-02, -1.1749e-01,\n",
       "        -3.4058e-01, -1.8445e-01, -3.1174e-02, -6.8359e-02,  3.7744e-01,\n",
       "         7.7820e-02,  2.0154e-01, -7.4280e-02,  1.0089e-01,  9.7229e-02,\n",
       "        -1.9690e-01,  2.6636e-01, -4.8027e-03,  2.1143e-01, -3.2568e-01,\n",
       "         4.0955e-02,  3.0835e-01,  1.3153e-02,  3.2812e-01,  1.5915e-02,\n",
       "        -5.1422e-02,  2.2327e-01,  1.9788e-01,  1.7859e-01, -1.0443e-01,\n",
       "         5.1819e-02, -1.0797e-01, -1.0333e-01,  2.8711e-01, -9.6497e-02,\n",
       "        -1.9629e-01,  2.8458e-02,  9.4543e-02,  2.1338e-01,  2.7563e-01,\n",
       "         2.0996e-01, -1.4734e-01, -1.6943e-01,  2.0471e-01,  6.1836e-03,\n",
       "         2.0081e-02, -2.5977e-01, -1.2646e-01, -3.3374e-01,  4.0161e-02,\n",
       "         3.5889e-01,  3.1647e-02, -6.7688e-02, -1.8967e-02, -4.5258e-02,\n",
       "         1.3318e-01,  3.7427e-01,  1.0754e-01, -3.5980e-02,  6.3818e-01,\n",
       "         1.9312e-01,  7.6111e-02,  2.3842e-04, -1.2245e-02, -1.5967e-01,\n",
       "         1.3000e-01,  1.0272e-01, -8.2947e-02,  1.4453e-01, -5.1910e-02,\n",
       "        -1.6846e-01,  6.7578e-01,  8.5693e-01, -3.5815e-01,  1.9189e-01,\n",
       "        -2.2632e-01,  1.2952e-01,  8.8257e-02, -1.9385e-01, -3.5767e-01,\n",
       "        -2.7832e-01,  1.0413e-01,  1.7822e-02, -1.8890e-02, -3.0945e-02,\n",
       "        -2.7173e-01, -2.6831e-01, -8.9355e-02, -1.5601e-01, -1.0931e-01,\n",
       "         3.4131e-01,  9.8572e-02,  3.7500e-01, -9.4116e-02,  4.1077e-02,\n",
       "        -1.5613e-01,  3.7891e-01, -3.6438e-02, -3.0005e-01,  1.1450e-01,\n",
       "         1.7598e+00, -2.9388e-02,  6.6101e-02, -4.0150e-04,  2.0239e-01,\n",
       "         1.1603e-01, -5.5176e-02, -3.6560e-02, -1.2433e-01,  1.3892e-01,\n",
       "         3.6108e-01, -8.3801e-02,  2.1851e-01, -7.7271e-02,  4.3274e-02,\n",
       "         1.3818e-01, -2.4011e-01, -2.7979e-01, -1.0046e-01, -1.4233e-01,\n",
       "        -3.9368e-02, -2.4304e-01,  1.4697e-01, -3.7670e-03, -2.7832e-02,\n",
       "        -4.0955e-02, -3.8330e-01,  6.9946e-02,  2.1765e-01,  8.4900e-02,\n",
       "        -2.9639e-01,  3.2861e-01,  4.0308e-01, -1.1096e-01, -7.5562e-02,\n",
       "         3.6072e-02, -9.1492e-02, -1.9006e-01,  1.7822e-01, -8.9722e-02,\n",
       "         6.6895e-02,  1.4539e-01, -2.4139e-02, -1.9116e-01,  2.0618e-01,\n",
       "        -1.5710e-01, -4.6194e-05, -6.6895e-02,  9.9243e-02, -2.0190e-01,\n",
       "         3.4729e-02,  1.2952e-01,  8.6133e-01, -1.9324e-01,  1.3863e-02,\n",
       "        -6.1676e-02,  1.6406e-01, -4.8004e-02, -2.6392e-01,  1.6821e-01,\n",
       "        -5.7159e-02,  9.4055e-02, -1.3708e-01,  1.3098e-01, -2.0764e-01,\n",
       "        -3.3667e-01, -1.2524e-01, -2.2278e-01, -4.1565e-02, -1.3879e-01,\n",
       "        -2.5537e-01, -2.6260e-02, -5.2539e-01,  1.3184e-01, -3.7720e-02,\n",
       "         2.4561e-01,  1.0980e-01, -7.3547e-02,  3.3179e-01,  1.7773e-01,\n",
       "        -1.5393e-01,  1.6382e-01,  3.7842e-02, -2.3010e-01, -9.6863e-02,\n",
       "         2.5537e-01, -2.7686e-01, -2.2522e-01, -1.7725e-01, -8.3862e-02,\n",
       "         5.0201e-02,  1.5247e-01, -1.2524e-01,  2.5586e-01, -8.1970e-02,\n",
       "        -4.4434e-01,  2.6660e-01, -1.2061e-01, -8.1665e-02, -1.2610e-01,\n",
       "         1.0394e-01, -3.2007e-01,  2.0105e-01,  1.1005e-01, -4.9561e-02,\n",
       "        -6.6040e-02,  1.6333e-01,  1.7319e-02, -1.6296e-01, -2.0178e-01,\n",
       "         3.6426e-01,  3.4131e-01, -1.4697e-01, -1.8896e-01,  1.1243e-01,\n",
       "         2.1436e-01,  2.4597e-01, -2.9614e-01,  8.4351e-02,  8.8318e-02,\n",
       "         4.6631e-01,  9.4360e-02,  1.0055e+01,  2.4170e-01,  1.4368e-01,\n",
       "        -1.9852e-02,  1.4551e-01,  3.0151e-02,  4.1351e-02,  4.1885e-03,\n",
       "         4.4861e-02,  6.8213e-01, -1.0901e-01, -5.4688e-02,  2.2705e-02,\n",
       "         8.9050e-02, -1.9800e-01,  2.0581e-01, -5.8075e-02, -6.3623e-01,\n",
       "         6.9397e-02, -1.7163e-01,  5.2441e-01,  2.3816e-01,  1.3379e-01,\n",
       "         1.3046e-02,  1.1273e-01, -4.3526e-03, -1.5732e-02,  5.2399e-02,\n",
       "         2.0264e-01,  1.6431e-01,  3.8696e-02,  2.1899e-01, -1.8384e-01,\n",
       "         2.3392e-02, -2.9037e-02, -1.3196e-01,  2.5854e-01, -1.1078e-01,\n",
       "        -2.7930e-01, -8.2397e-02,  1.5564e-01,  5.5786e-02,  2.6749e-02,\n",
       "        -9.8389e-02, -5.0928e-01,  1.5259e-01, -2.4829e-01,  2.0703e-01,\n",
       "        -1.6589e-01,  2.9221e-02, -5.7831e-02,  8.0627e-02,  1.0583e-01,\n",
       "        -2.3523e-01,  1.1224e-01,  1.2317e-01, -1.2152e-01,  1.9824e-01,\n",
       "        -3.1738e-01,  1.2848e-02,  3.0493e-01, -3.6426e-01, -1.1487e-01,\n",
       "        -5.4932e-02,  7.5745e-02, -2.8320e-01,  1.7139e-01,  9.8694e-02,\n",
       "         4.1675e-01,  7.0679e-02, -1.6129e-02, -1.0571e-01, -3.2520e-01,\n",
       "         4.0137e-01,  1.9177e-01, -7.1228e-02, -2.5610e-01, -4.1895e-01,\n",
       "        -1.6199e-01, -4.8065e-02,  2.2422e+00,  2.4683e-01, -2.1179e-01,\n",
       "        -8.6670e-02, -4.2407e-01,  1.0643e-02, -3.5693e-01,  3.3813e-02,\n",
       "        -2.4426e-01,  2.7661e-01, -3.4424e-01,  1.1444e-01, -2.1936e-01,\n",
       "         2.0703e-01, -3.3643e-01,  5.0720e-02,  3.4448e-01, -9.5947e-02,\n",
       "         5.6343e-03,  2.0422e-01, -3.6285e-02,  1.9910e-01, -3.2275e-01,\n",
       "         2.3755e-01,  2.9370e-01,  4.4678e-02, -2.0520e-01,  9.4727e-02,\n",
       "         3.4741e-01, -1.8384e-01, -2.6642e-02, -1.0431e-01,  2.3254e-01,\n",
       "        -2.6343e-01,  2.0068e-01, -2.8613e-01,  2.7271e-01,  2.8662e-01,\n",
       "         2.0203e-02,  1.2002e+00, -1.8396e-01,  4.8676e-02, -7.9590e-02,\n",
       "         3.9478e-01,  3.1299e-01,  1.1639e-01,  9.2712e-02,  4.0942e-01,\n",
       "        -8.0505e-02,  8.5526e-03,  2.4646e-01,  6.2378e-02,  1.6956e-01,\n",
       "        -1.5320e-01, -2.4170e-01, -7.1960e-02, -2.0508e-01, -2.8955e-01,\n",
       "         1.7346e-01, -2.1045e-01, -1.0040e-01, -1.3904e-01, -1.0596e-01,\n",
       "        -5.1086e-02,  2.9297e-02,  2.7002e-01,  2.6782e-01,  4.0527e-02,\n",
       "         7.9346e-02, -7.3471e-03], dtype=torch.float16)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbsFeat[\"beat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beat drum'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## part\n",
    "data_sample[7]+' '+data_sample[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 224, 224])\n",
      "torch.Size([1, 224, 224])\n",
      "[1500, 1297]\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "beat\n",
      "drum\n",
      "../../AGD20K/Seen/testset/egocentric/beat/drum/drum_000346.jpg\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "##      0         1         2               3           4          5         6        7     8      9     10   11\n",
    "##  input_img, gt_mask, gt_mask_prob, input_shape, sent_feat, noun_feat, part_feat, verb, noun, input_p, vid, nid\n",
    "data_sample=eval_data_loader.dataset[0]\n",
    "print(len(data_sample))\n",
    "print(data_sample[0].shape)\n",
    "print(data_sample[1].shape)\n",
    "print(data_sample[2].shape)\n",
    "print(data_sample[3])\n",
    "print(data_sample[4].shape)\n",
    "print(data_sample[5].shape)\n",
    "print(data_sample[6].shape)\n",
    "print(data_sample[7])\n",
    "print(data_sample[8])\n",
    "print(data_sample[9])\n",
    "print(data_sample[10])\n",
    "print(data_sample[11])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, CLIPModel\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# inputs = tokenizer([\"beat\"], padding=True, return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     text_features2 = model.get_text_features(**inputs)\n",
    "# text_features2.squeeze(0).cpu().type(torch.float16)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_ckpt = \"./ViT-B-16.pt\"\n",
    "clip_model, preprocess = clip.load(clip_ckpt, device)\n",
    "\n",
    "# text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(\"beat\") ]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text_inputs)\n",
    "\n",
    "# # Pick the top 5 most similar labels for the image\n",
    "# text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0025,  0.0491,  0.1433, -0.0350, -0.4016,  0.0185, -0.2386,  0.0314,\n",
       "        -0.0471,  0.0494], dtype=torch.float16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.squeeze(0).cpu()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0029,  0.0479,  0.1428, -0.0352, -0.4011,  0.0199, -0.2383,  0.0312,\n",
       "        -0.0462,  0.0504, -0.0158,  0.1730, -0.0890,  0.0467,  0.1791, -0.1038,\n",
       "        -0.1213, -0.8071,  0.0277, -0.1987, -0.1443,  0.2258, -0.0798,  0.0709,\n",
       "         0.0082,  0.0308,  0.1114,  0.2749, -0.2085, -0.0732,  0.1707,  0.0383,\n",
       "        -0.2014, -0.1412,  0.4629,  0.4338, -0.1146, -0.4390,  0.4363,  0.1201],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbsFeat[\"beat\"][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### img path\n",
    "1. cup - drink_with\n",
    "    AGD20K/Unseen/testset/egocentric/drink_with/cup/cup_002062.jpg\n",
    "'''\n",
    "img_path_list=[\"../../AGD20K/Unseen/testset/egocentric/drink_with/cup/cup_002062.jpg\",\n",
    "            #   \"../../AGD20K/Unseen/testset/egocentric/drink_with/cup/cup_002062.jpg\",\n",
    "            #   \"../../AGD20K/Unseen/testset/egocentric/drink_with/cup/cup_002062.jpg\",\n",
    "            #   \"../../AGD20K/Unseen/testset/egocentric/drink_with/cup/cup_002062.jpg\",\n",
    "            #   \"../../AGD20K/Unseen/testset/egocentric/drink_with/cup/cup_002062.jpg\",\n",
    "            #   \"../../AGD20K/Unseen/testset/egocentric/drink_with/cup/cup_002062.jpg\",\n",
    "              ]\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "transform_noresize = transforms.Compose([\n",
    "            _convert_image_to_rgb,\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (224, 224, 3) torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'addWeighted'\n> Overload resolution failed:\n>  - src2 is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src2'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 85\u001b[0m\n\u001b[1;32m     78\u001b[0m         ii \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m     79\u001b[0m             input_image[bid:bid\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     80\u001b[0m             size\u001b[38;5;241m=\u001b[39mgt_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:],\n\u001b[1;32m     81\u001b[0m             mode\u001b[38;5;241m=\u001b[39mINTERPOLATE_MODE,\n\u001b[1;32m     82\u001b[0m         )\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m*\u001b[39mgt_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m## visualize with gt\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m         \u001b[43mplot_annotation_with_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mii\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgt_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgt_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult on AGD: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKLD=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvall_kld\u001b[38;5;241m/\u001b[39mvall_num_sum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SIM=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvall_sim\u001b[38;5;241m/\u001b[39mvall_num_sum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, NSS=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvall_nss\u001b[38;5;241m/\u001b[39mvall_num_sum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 91\u001b[0m, in \u001b[0;36mplot_annotation_with_gt\u001b[0;34m(image, heatmap, gt, alpha, name)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# print(processed_heatmap.shape, image.shape)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# assert processed_heatmap.shape == image.shape\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape:\u001b[39m\u001b[38;5;124m\"\u001b[39m,processed_heatmap\u001b[38;5;241m.\u001b[39mshape, image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 91\u001b[0m overlay \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddWeighted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_heatmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO: [:, :, ::-1]\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m### Process the ground truth mask\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Plot the overlay of heatmap on the target image.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m processed_gt \u001b[38;5;241m=\u001b[39m gt \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(gt)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'addWeighted'\n> Overload resolution failed:\n>  - src2 is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src2'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    img_path=img_path_list[0]\n",
    "    gt_pth=img_path.replace(\".jpg\", \".png\").replace(\"egocentric\", \"GT\")\n",
    "    ### 1-a) text feature\n",
    "    # text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "    text_inputs = torch.cat([clip.tokenize(\"beat\") ]).to(device)\n",
    "    text_features = clip_model.encode_text(text_inputs)\n",
    "    \n",
    "    ### 1-b) image feature\n",
    "    input_img = Image.open(img_path)\n",
    "    input_shape = [input_img.size[1], input_img.size[0]]\n",
    "    input_img_no_resize = transform_noresize(input_img)\n",
    "    \n",
    "    if config[\"img_size\"]:\n",
    "        input_image = F.interpolate(\n",
    "            input_img_no_resize.unsqueeze(0),\n",
    "            size=config[\"img_size\"],\n",
    "            mode=\"bilinear\",\n",
    "        ).squeeze(0)\n",
    "    else:\n",
    "        input_image = F.interpolate(\n",
    "            input_img_no_resize.unsqueeze(0),\n",
    "            size=224,\n",
    "            mode=\"bilinear\",\n",
    "        ).squeeze(0)\n",
    "    input_image = input_image.unsqueeze(0)\n",
    "\n",
    "    ## 2) gt mask process\n",
    "    gt_ori = torch.tensor(np.array(Image.open(gt_pth))).float().reshape(1, *input_shape)\n",
    "\n",
    "    if config[\"img_size\"]:\n",
    "        gt = F.interpolate(\n",
    "            gt_ori.unsqueeze(0),\n",
    "            size=config[\"img_size\"],\n",
    "            mode=\"bilinear\",\n",
    "        ).squeeze(0)\n",
    "    else:\n",
    "        gt = gt_ori\n",
    "    # gt=gt.unsqueeze(0)\n",
    "\n",
    "    if gt.max() == 0: # Seen/testset/GT/hold/bottle/bottle_000341.png\n",
    "        gt = torch.ones_like(gt)\n",
    "    gt_mask = gt / gt.max()\n",
    "    gt_mask_prob = gt / gt.sum()\n",
    "\n",
    "    ### 3) model inference\n",
    "    aff_res, _, _ = model(\n",
    "        input_image, text_features,\n",
    "    )\n",
    "    pred = aff_res.detach()\n",
    "    \n",
    "    r_pred = F.interpolate(\n",
    "        pred, \n",
    "        size=gt_mask.shape[-2:],\n",
    "        mode=INTERPOLATE_MODE,\n",
    "    )\n",
    "    \n",
    "    gt_prob = gt_mask_prob.cuda().reshape(len(pred), -1)\n",
    "    r_prob = F.softmax(r_pred.reshape(len(pred), -1), dim=1)\n",
    "    \n",
    "    kld_per_sample = KLD(r_prob, gt_prob, \"none\").sum(dim=1)\n",
    "    kld = kld_per_sample.sum()\n",
    "    sim = SIM(r_prob, gt_prob) * len(pred)\n",
    "    nss = NSS(r_prob, gt_prob) * len(pred)\n",
    "    vall_kld += kld\n",
    "    vall_sim += sim\n",
    "    vall_nss += nss\n",
    "    vall_num += 1\n",
    "    vall_num_sum += len(pred)\n",
    "    \n",
    "    verbs = None\n",
    "    nouns = None\n",
    "    \n",
    "    \n",
    "    for bid in range(len(r_pred)):\n",
    "        pp = r_prob[bid] / r_prob[bid].max()\n",
    "        \n",
    "        ii = F.interpolate(\n",
    "            input_image[bid:bid+1], \n",
    "            size=gt_mask.shape[-2:],\n",
    "            mode=INTERPOLATE_MODE,\n",
    "        ).reshape(3, *gt_mask.shape[-2:])\n",
    "        \n",
    "        ## visualize with gt\n",
    "        plot_annotation_with_gt(ii, \n",
    "                        pp.reshape(*gt_mask[bid].shape).detach().unsqueeze(0).cpu().numpy().transpose(1, 2, 0), \n",
    "                        gt_mask[bid].unsqueeze(0).cpu().numpy().transpose(1, 2, 0), \n",
    "                        name=None)\n",
    "        \n",
    "    \n",
    "print(\n",
    "    f\"Result on AGD: \\nKLD={vall_kld/vall_num_sum}, SIM={vall_sim/vall_num_sum}, NSS={vall_nss/vall_num_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gt_mask[bid].unsqueeze(0).cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.reshape(*gt_mask[bid].shape).detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_mask[bid].cpu().numpy().transpose(1, 2, 0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsag-plsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
