{'work_dir': 'logs2/5_ours_rl0.0001_wd0.01_lr_step20_gamma0.5_noun1-part0.1-context_sim0.01_exo0_direct_feature_context2', 'deterministic': False, 'data_dir': '../../AGD20K', 'GT_data_dir': '../../06_Affordance-R1/Affordance-R1/vis_results_refine_anno_v2/sampled/AGD20K_refine/', 'load': {'encoder_ckpt': './ViT-B-16.pt', 'all_ckpt': None}, 'batch_size': 32, 'img_size': 224, 'split_type': 'Seen', 'num_exo': 0, 'PL_mode': 'refined', 'aug4imgRatio': 0.5, 'optimizer': {'lr': 0.0001, 'lr_encoder_coeff': 0.1, 'betas': [0.9, 0.95], 'wd': 0.01, 'max_iter': 5000000, 'lr_step': 20, 'lr_gamma': 0.5, 'num_epochs': 40, 'accum_iter': 2, 'sche_type': 'step'}, 'model': {'encoder_type': 'CLIP', 'encoder_params': {'width': 768, 'layers': 12, 'heads': 12, 'output_dim': 512}, 'decoder_embed_dim': 512, 'pred_model_type': 'SAM', 'pred_decoder_args': {'mlp_dim': 2048, 'depth': 2, 'use_up': 2, 'use_additional_token': True, 'conv_first': True}, 'margin': 0.1}, 'loss': {'kl_loss_coeff': 1.0, 'sim_loss_coeff': 10.0, 'exo_cls_coeff': 10.0, 'noun_sim_coeff': 1.0, 'part_sim_coeff': 0.1, 'context_sim_coeff': 0.01}}
======================Config:======================
GT_data_dir: ../../06_Affordance-R1/Affordance-R1/vis_results_refine_anno_v2/sampled/AGD20K_refine/
PL_mode: refined
aug4imgRatio: 0.5
batch_size: 32
data_dir: ../../AGD20K
deterministic: false
img_size: 224
load:
  all_ckpt: null
  encoder_ckpt: ./ViT-B-16.pt
loss:
  context_sim_coeff: 0.01
  exo_cls_coeff: 10.0
  kl_loss_coeff: 1.0
  noun_sim_coeff: 1.0
  part_sim_coeff: 0.1
  sim_loss_coeff: 10.0
model:
  decoder_embed_dim: 512
  encoder_params:
    heads: 12
    layers: 12
    output_dim: 512
    width: 768
  encoder_type: CLIP
  margin: 0.1
  pred_decoder_args:
    conv_first: true
    depth: 2
    mlp_dim: 2048
    use_additional_token: true
    use_up: 2
  pred_model_type: SAM
num_exo: 0
optimizer:
  accum_iter: 2
  betas:
  - 0.9
  - 0.95
  lr: 0.0001
  lr_encoder_coeff: 0.1
  lr_gamma: 0.5
  lr_step: 20
  max_iter: 5000000
  num_epochs: 40
  sche_type: step
  wd: 0.01
split_type: Seen
work_dir: logs2/5_ours_rl0.0001_wd0.01_lr_step20_gamma0.5_noun1-part0.1-context_sim0.01_exo0_direct_feature_context2

Set random seed to 0, deterministic: False
[], [] are misaligned params in CLIP Encoder
#Params: 114358820
#Encoder Params: 86192640
#Final Decoder Params: 12152576
Model:
ModelAGDsup(
  (encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (verb_fuser): Affordance_Decoder(
    (regressor_blocks): ModuleList(
      (0): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03333333507180214)
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06666666269302368)
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.10000000149011612)
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pred_decoder): SAM_Decoder_Simple(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0): TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=512, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=512, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
        )
        (1): TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=512, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=512, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=512, out_features=256, bias=True)
        (k_proj): Linear(in_features=512, out_features=256, bias=True)
        (v_proj): Linear(in_features=512, out_features=256, bias=True)
        (out_proj): Linear(in_features=256, out_features=512, bias=True)
      )
      (norm_final_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (add_token): Embedding(1, 512)
    (output_upscaling): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LayerNorm2d()
      (2): GELU()
      (3): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU()
      (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
      (7): GELU()
    )
    (output_hypernetworks_mlp): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=512, bias=True)
        (2): Linear(in_features=512, out_features=64, bias=True)
      )
    )
    (pe_layer): PositionEmbeddingRandom()
  )
  (exo_cls): Sequential(
    (0): Linear(in_features=512, out_features=256, bias=True)
    (1): GELU()
    (2): Linear(in_features=256, out_features=36, bias=True)
  )
  (noun_transform): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (reason): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (context_aware): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (active_transform): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (passive_transform): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
)
Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.95]
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 0.01

Parameter Group 1
    amsgrad: False
    betas: [0.9, 0.95]
    eps: 1e-08
    initial_lr: 1e-05
    lr: 1e-05
    weight_decay: 0.01
)
============Training Epoch 0============
Training loss: 3.2259907126426697, KL loss: 2.243740564584732, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.8957860320806503, Part sim loss: 0.7702879011631012 
Context_simm_loss: 0.9435327112674713
learning rate:0.0001

Result on AGD: 
KLD=2.2539162635803223, SIM=0.22390547394752502, NSS=1.5322480201721191
noun sim: 0.7580174207687378, part sim: 0.5450816988945008
New best KLD: 2.2539162635803223, 0.22390547394752502, 1.5322480201721191
============Training Epoch 1============
Training loss: 2.286158448457718, KL loss: 1.6199888944625855, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.6166407376527786, Part sim loss: 0.42173128575086594 
Context_simm_loss: 0.7355676710605621
learning rate:0.0001

Result on AGD: 
KLD=1.9018638134002686, SIM=0.2857706844806671, NSS=2.081448793411255
noun sim: 0.5604694247245788, part sim: 0.29026645720005034
New best KLD: 1.9018638134002686, 0.2857706844806671, 2.081448793411255
============Training Epoch 2============
Training loss: 1.690090548992157, KL loss: 1.2195311695337296, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.44297589510679247, Part sim loss: 0.22458236217498778 
Context_simm_loss: 0.5125249549746513
learning rate:0.0001

Result on AGD: 
KLD=1.8948732614517212, SIM=0.258434921503067, NSS=2.3563480377197266
noun sim: 0.3957367241382599, part sim: 0.15210142731666565
New best KLD: 1.8948732614517212, 0.258434921503067, 2.3563480377197266
============Training Epoch 3============
Training loss: 1.3349735379219054, KL loss: 1.0137200981378556, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.3054623082280159, Part sim loss: 0.12894570752978324 
Context_simm_loss: 0.28965504467487335
learning rate:0.0001

Result on AGD: 
KLD=1.6478677988052368, SIM=0.3167373239994049, NSS=2.713338851928711
noun sim: 0.29336275458335875, part sim: 0.1026065081357956
New best KLD: 1.6478677988052368, 0.3167373239994049, 2.713338851928711
============Training Epoch 4============
Training loss: 1.0973450809717178, KL loss: 0.8564730018377305, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.2291916213929653, Part sim loss: 0.10292459279298782 
Context_simm_loss: 0.1388008665293455
learning rate:0.0001

Result on AGD: 
KLD=1.7179591655731201, SIM=0.3121366500854492, NSS=2.5094282627105713
noun sim: 0.2258087784051895, part sim: 0.09030390605330467
============Training Epoch 5============
Training loss: 0.9318032026290893, KL loss: 0.7321075111627579, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.18944339454174042, Part sim loss: 0.09384112767875194 
Context_simm_loss: 0.0868182934820652
learning rate:0.0001

Result on AGD: 
KLD=1.7844479084014893, SIM=0.2813833951950073, NSS=2.462482452392578
noun sim: 0.18897168934345246, part sim: 0.08416477143764496
============Training Epoch 6============
Training loss: 0.834202167391777, KL loss: 0.6636579602956771, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.16098158955574035, Part sim loss: 0.08827684670686722 
Context_simm_loss: 0.0734937161207199
learning rate:0.0001

Result on AGD: 
KLD=1.7126065492630005, SIM=0.3154664933681488, NSS=2.5111916065216064
noun sim: 0.16349732279777526, part sim: 0.0788348525762558
============Training Epoch 7============
Training loss: 0.6892899811267853, KL loss: 0.5332446917891502, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.14689656496047973, Part sim loss: 0.08475332222878933 
Context_simm_loss: 0.06733941286802292
learning rate:0.0001

Result on AGD: 
KLD=1.813201665878296, SIM=0.32279759645462036, NSS=2.2900726795196533
noun sim: 0.1489718347787857, part sim: 0.07664408534765244
============Training Epoch 8============
Training loss: 0.6493190944194793, KL loss: 0.5043628200888634, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.13594185635447503, Part sim loss: 0.08370066657662392 
Context_simm_loss: 0.06443451885133981
learning rate:0.0001

Result on AGD: 
KLD=1.6996744871139526, SIM=0.3204003870487213, NSS=2.5394153594970703
noun sim: 0.13764411062002183, part sim: 0.07634143531322479
============Training Epoch 9============
Training loss: 0.609920984506607, KL loss: 0.4719974398612976, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.12904645167291165, Part sim loss: 0.08248880598694086 
Context_simm_loss: 0.06282095201313495
learning rate:0.0001

Result on AGD: 
KLD=1.7954622507095337, SIM=0.32988107204437256, NSS=2.3317673206329346
noun sim: 0.1307205229997635, part sim: 0.07533120959997178
============Training Epoch 10============
Training loss: 0.5993137747049332, KL loss: 0.4650504276156425, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.12546928264200688, Part sim loss: 0.08175094686448574 
Context_simm_loss: 0.06189690995961428
learning rate:0.0001

Result on AGD: 
KLD=1.757372498512268, SIM=0.30663853883743286, NSS=2.421308994293213
noun sim: 0.1272047355771065, part sim: 0.07506824135780335
============Training Epoch 11============
Training loss: 0.5380318522453308, KL loss: 0.40680472254753114, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.12240491844713688, Part sim loss: 0.0820658765733242 
Context_simm_loss: 0.06156285982578993
learning rate:0.0001

Result on AGD: 
KLD=1.6590359210968018, SIM=0.30908697843551636, NSS=2.711341142654419
noun sim: 0.12419845908880234, part sim: 0.07432080581784248
============Training Epoch 12============
Training loss: 0.5129942312836647, KL loss: 0.38436962217092513, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11987446621060371, Part sim loss: 0.0813832312822342 
Context_simm_loss: 0.06118191219866276
learning rate:0.0001

Result on AGD: 
KLD=1.7200136184692383, SIM=0.3407008945941925, NSS=2.4265365600585938
noun sim: 0.12119937539100648, part sim: 0.0738543450832367
============Training Epoch 13============
Training loss: 0.48046432584524157, KL loss: 0.35303030610084535, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11879833452403546, Part sim loss: 0.0803149051964283 
Context_simm_loss: 0.060420841351151465
learning rate:0.0001

Result on AGD: 
KLD=1.8016116619110107, SIM=0.3227806091308594, NSS=2.3315930366516113
noun sim: 0.11793017089366913, part sim: 0.07350787222385406
============Training Epoch 14============
Training loss: 0.4715897724032402, KL loss: 0.3471300333738327, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11581530384719371, Part sim loss: 0.08041901588439941 
Context_simm_loss: 0.0602530250325799
learning rate:0.0001

Result on AGD: 
KLD=1.8353691101074219, SIM=0.33433622121810913, NSS=2.271664619445801
noun sim: 0.11422844827175141, part sim: 0.07329975441098213
============Training Epoch 15============
Training loss: 0.42945791184902193, KL loss: 0.3071424312889576, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11366586312651634, Part sim loss: 0.08049473650753498 
Context_simm_loss: 0.06001457329839468
learning rate:0.0001

Result on AGD: 
KLD=1.8505202531814575, SIM=0.3117847144603729, NSS=2.2401881217956543
noun sim: 0.11127403229475022, part sim: 0.07316350564360619
============Training Epoch 16============
Training loss: 0.4278295338153839, KL loss: 0.3075597912073135, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11169054470956326, Part sim loss: 0.07981908656656742 
Context_simm_loss: 0.05972926542162895
learning rate:0.0001

Result on AGD: 
KLD=1.8426347970962524, SIM=0.3437034487724304, NSS=2.3166205883026123
noun sim: 0.10882256478071213, part sim: 0.07310879752039909
============Training Epoch 17============
Training loss: 0.4452703967690468, KL loss: 0.32642441987991333, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11020266003906727, Part sim loss: 0.08046323657035828 
Context_simm_loss: 0.05970028601586819
learning rate:0.0001

Result on AGD: 
KLD=1.5959479808807373, SIM=0.3368593454360962, NSS=2.8406825065612793
noun sim: 0.10814397633075715, part sim: 0.0724511943757534
New best KLD: 1.5959479808807373, 0.3368593454360962, 2.8406825065612793
============Training Epoch 18============
Training loss: 0.3970002010464668, KL loss: 0.27847787365317345, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10989692434668541, Part sim loss: 0.08021091222763062 
Context_simm_loss: 0.06043190285563469
learning rate:0.0001

Result on AGD: 
KLD=1.809415578842163, SIM=0.33094167709350586, NSS=2.307352304458618
noun sim: 0.10742084383964538, part sim: 0.07244031727313996
============Training Epoch 19============
Training loss: 0.37530122250318526, KL loss: 0.2575936898589134, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10913148894906044, Part sim loss: 0.07978557832539082 
Context_simm_loss: 0.05974860806018114
learning rate:5e-05

Result on AGD: 
KLD=1.9498720169067383, SIM=0.3307964503765106, NSS=2.1457202434539795
noun sim: 0.10636981129646302, part sim: 0.07176361680030822
============Training Epoch 20============
Training loss: 0.3712144777178764, KL loss: 0.2549499697983265, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10769186727702618, Part sim loss: 0.07976566553115845 
Context_simm_loss: 0.05960786435753107
learning rate:5e-05

Result on AGD: 
KLD=1.7170071601867676, SIM=0.32504701614379883, NSS=2.5101099014282227
noun sim: 0.10561456382274628, part sim: 0.07178929597139358
============Training Epoch 21============
Training loss: 0.34559164494276046, KL loss: 0.2295190081000328, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10752571895718574, Part sim loss: 0.07953283861279488 
Context_simm_loss: 0.05936308279633522
learning rate:5e-05

Result on AGD: 
KLD=1.8383400440216064, SIM=0.3253262937068939, NSS=2.287790298461914
noun sim: 0.10576272904872894, part sim: 0.07169334143400193
============Training Epoch 22============
Training loss: 0.3463906139135361, KL loss: 0.23046991154551505, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10742162019014359, Part sim loss: 0.07899441830813884 
Context_simm_loss: 0.05996404550969601
learning rate:5e-05

Result on AGD: 
KLD=1.813572645187378, SIM=0.33760368824005127, NSS=2.422574043273926
noun sim: 0.10466697812080383, part sim: 0.07148620635271072
============Training Epoch 23============
Training loss: 0.3343545958399773, KL loss: 0.2191382147371769, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10671507008373737, Part sim loss: 0.07912050299346447 
Context_simm_loss: 0.05892648063600063
learning rate:5e-05

Result on AGD: 
KLD=1.7923059463500977, SIM=0.32542458176612854, NSS=2.398747205734253
noun sim: 0.1048389732837677, part sim: 0.07145245149731635
============Training Epoch 24============
Training loss: 0.3131165400147438, KL loss: 0.1978192523121834, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10679815486073493, Part sim loss: 0.0790641326457262 
Context_simm_loss: 0.05927177015691996
learning rate:5e-05

Result on AGD: 
KLD=1.793149471282959, SIM=0.32866203784942627, NSS=2.4152004718780518
noun sim: 0.10441693067550659, part sim: 0.07130436673760414
============Training Epoch 25============
Training loss: 0.3096108563244343, KL loss: 0.19467715695500373, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10644013285636902, Part sim loss: 0.07901800610125065 
Context_simm_loss: 0.05917726494371891
learning rate:5e-05

Result on AGD: 
KLD=1.9059348106384277, SIM=0.32444074749946594, NSS=2.2481462955474854
noun sim: 0.10387535393238068, part sim: 0.07105949372053147
============Training Epoch 26============
Training loss: 0.2935997791588306, KL loss: 0.17875479198992253, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10635191835463047, Part sim loss: 0.07898358777165412 
Context_simm_loss: 0.0594711834564805
learning rate:5e-05

Result on AGD: 
KLD=1.7750431299209595, SIM=0.3362073302268982, NSS=2.424630641937256
noun sim: 0.10422970205545426, part sim: 0.07094538807868958
============Training Epoch 27============
Training loss: 0.2850001595914364, KL loss: 0.1703496254980564, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.1062712449580431, Part sim loss: 0.07788226343691348 
Context_simm_loss: 0.059105978906154634
learning rate:5e-05

Result on AGD: 
KLD=1.8570876121520996, SIM=0.32627129554748535, NSS=2.321505546569824
noun sim: 0.10295814126729966, part sim: 0.07091505005955696
============Training Epoch 28============
Training loss: 0.27610678896307944, KL loss: 0.16198236159980298, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10562768802046776, Part sim loss: 0.07901921570301056 
Context_simm_loss: 0.05948184691369533
learning rate:5e-05

Result on AGD: 
KLD=1.8467109203338623, SIM=0.3310191333293915, NSS=2.3508076667785645
noun sim: 0.10275879800319672, part sim: 0.07089498117566109
============Training Epoch 29============
Training loss: 0.304982278496027, KL loss: 0.19128792807459832, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10521897971630097, Part sim loss: 0.0788346290588379 
Context_simm_loss: 0.0591913178563118
learning rate:5e-05

Result on AGD: 
KLD=1.7673171758651733, SIM=0.3321548402309418, NSS=2.4500694274902344
noun sim: 0.10262310206890106, part sim: 0.07091923207044601
============Training Epoch 30============
Training loss: 0.29175459370017054, KL loss: 0.178119557723403, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10519995763897896, Part sim loss: 0.07842779196798802 
Context_simm_loss: 0.059229314886033536
learning rate:5e-05

Result on AGD: 
KLD=1.8295735120773315, SIM=0.34081193804740906, NSS=2.4048383235931396
noun sim: 0.10242293626070023, part sim: 0.07061692327260971
============Training Epoch 31============
Training loss: 0.2764361649751663, KL loss: 0.16274939179420472, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10518653728067875, Part sim loss: 0.07905654087662697 
Context_simm_loss: 0.059457927383482455
learning rate:5e-05

Result on AGD: 
KLD=1.8613090515136719, SIM=0.3098958134651184, NSS=2.3498566150665283
noun sim: 0.10236051827669143, part sim: 0.07079371958971023
============Training Epoch 32============
Training loss: 0.2776508338749409, KL loss: 0.1642603013664484, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10499147959053516, Part sim loss: 0.07807270623743534 
Context_simm_loss: 0.059177924506366254
learning rate:5e-05

Result on AGD: 
KLD=1.8623661994934082, SIM=0.3431514799594879, NSS=2.3639228343963623
noun sim: 0.10169662088155747, part sim: 0.07094781771302223
============Training Epoch 33============
Training loss: 0.2732205353677273, KL loss: 0.15998592637479306, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10485558733344078, Part sim loss: 0.0779083639383316 
Context_simm_loss: 0.05881867203861475
learning rate:5e-05

Result on AGD: 
KLD=1.8473317623138428, SIM=0.32924774289131165, NSS=2.3591079711914062
noun sim: 0.10185679197311401, part sim: 0.0708093836903572
============Training Epoch 34============
Training loss: 0.26804704740643504, KL loss: 0.1549636036157608, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10461345687508583, Part sim loss: 0.07877428866922856 
Context_simm_loss: 0.05925593674182892
learning rate:5e-05

Result on AGD: 
KLD=1.8309931755065918, SIM=0.33387500047683716, NSS=2.417661428451538
noun sim: 0.10200500786304474, part sim: 0.07069089859724045
============Training Epoch 35============
Training loss: 0.27699685618281367, KL loss: 0.16384853273630143, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10466484166681767, Part sim loss: 0.07887910194694996 
Context_simm_loss: 0.05955698043107986
learning rate:5e-05

Result on AGD: 
KLD=1.9095925092697144, SIM=0.3306778073310852, NSS=2.225598096847534
noun sim: 0.10147759914398194, part sim: 0.07076250314712525
============Training Epoch 36============
Training loss: 0.2649038203060627, KL loss: 0.1518106024712324, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.1046525340527296, Part sim loss: 0.07851471304893494 
Context_simm_loss: 0.05892149899154901
learning rate:5e-05

Result on AGD: 
KLD=1.8526904582977295, SIM=0.32982948422431946, NSS=2.380251407623291
noun sim: 0.10169373899698257, part sim: 0.07067089229822159
============Training Epoch 37============
Training loss: 0.257144308835268, KL loss: 0.144547775760293, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10421450473368168, Part sim loss: 0.07794800885021687 
Context_simm_loss: 0.058722897060215475
learning rate:5e-05

Result on AGD: 
KLD=1.821709156036377, SIM=0.3231160640716553, NSS=2.413879871368408
noun sim: 0.10126336961984635, part sim: 0.07037503868341446
============Training Epoch 38============
Training loss: 0.2535460993647575, KL loss: 0.14108230881392955, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10409939736127853, Part sim loss: 0.07774775940924883 
Context_simm_loss: 0.0589618606492877
learning rate:5e-05

Result on AGD: 
KLD=1.9044315814971924, SIM=0.3457699418067932, NSS=2.347507953643799
noun sim: 0.10124682486057282, part sim: 0.07069947198033333
============Training Epoch 39============
Training loss: 0.2543394587934017, KL loss: 0.14182534776628017, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10411849915981293, Part sim loss: 0.0780657459050417 
Context_simm_loss: 0.05890376511961222
learning rate:2.5e-05

Result on AGD: 
KLD=1.8347201347351074, SIM=0.33259710669517517, NSS=2.3945066928863525
noun sim: 0.10121638476848602, part sim: 0.07052983790636062
