{'work_dir': 'logs2/5_ours_rl0.0001_wd0.01_lr_step20_gamma0.5_noun1-part0.1-context_sim0.01_exo0_wo_active-token', 'deterministic': False, 'data_dir': '../../AGD20K', 'GT_data_dir': '../../06_Affordance-R1/Affordance-R1/vis_results_refine_anno_v2/sampled/AGD20K_refine/', 'load': {'encoder_ckpt': './ViT-B-16.pt', 'all_ckpt': None}, 'batch_size': 32, 'img_size': 224, 'split_type': 'Seen', 'num_exo': 0, 'PL_mode': 'refined', 'aug4imgRatio': 0.5, 'optimizer': {'lr': 0.0001, 'lr_encoder_coeff': 0.1, 'betas': [0.9, 0.95], 'wd': 0.01, 'max_iter': 5000000, 'lr_step': 20, 'lr_gamma': 0.5, 'num_epochs': 40, 'accum_iter': 2, 'sche_type': 'step'}, 'model': {'encoder_type': 'CLIP', 'encoder_params': {'width': 768, 'layers': 12, 'heads': 12, 'output_dim': 512}, 'decoder_embed_dim': 512, 'pred_model_type': 'SAM', 'pred_decoder_args': {'mlp_dim': 2048, 'depth': 2, 'use_up': 2, 'use_additional_token': True, 'conv_first': True}, 'margin': 0.1}, 'loss': {'kl_loss_coeff': 1.0, 'sim_loss_coeff': 10.0, 'exo_cls_coeff': 10.0, 'noun_sim_coeff': 1.0, 'part_sim_coeff': 0.1, 'context_sim_coeff': 0.01}}
======================Config:======================
GT_data_dir: ../../06_Affordance-R1/Affordance-R1/vis_results_refine_anno_v2/sampled/AGD20K_refine/
PL_mode: refined
aug4imgRatio: 0.5
batch_size: 32
data_dir: ../../AGD20K
deterministic: false
img_size: 224
load:
  all_ckpt: null
  encoder_ckpt: ./ViT-B-16.pt
loss:
  context_sim_coeff: 0.01
  exo_cls_coeff: 10.0
  kl_loss_coeff: 1.0
  noun_sim_coeff: 1.0
  part_sim_coeff: 0.1
  sim_loss_coeff: 10.0
model:
  decoder_embed_dim: 512
  encoder_params:
    heads: 12
    layers: 12
    output_dim: 512
    width: 768
  encoder_type: CLIP
  margin: 0.1
  pred_decoder_args:
    conv_first: true
    depth: 2
    mlp_dim: 2048
    use_additional_token: true
    use_up: 2
  pred_model_type: SAM
num_exo: 0
optimizer:
  accum_iter: 2
  betas:
  - 0.9
  - 0.95
  lr: 0.0001
  lr_encoder_coeff: 0.1
  lr_gamma: 0.5
  lr_step: 20
  max_iter: 5000000
  num_epochs: 40
  sche_type: step
  wd: 0.01
split_type: Seen
work_dir: logs2/5_ours_rl0.0001_wd0.01_lr_step20_gamma0.5_noun1-part0.1-context_sim0.01_exo0_wo_active-token

Set random seed to 0, deterministic: False
[], [] are misaligned params in CLIP Encoder
#Params: 114358820
#Encoder Params: 86192640
#Final Decoder Params: 12152576
Model:
ModelAGDsup(
  (encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (verb_fuser): Affordance_Decoder(
    (regressor_blocks): ModuleList(
      (0): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03333333507180214)
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06666666269302368)
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): RegressorBlock(
        (norm1_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm1_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2_cross): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn): CrossAttention(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.10000000149011612)
        (mlp_cross): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (pred_decoder): SAM_Decoder_Simple(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0): TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=512, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=512, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
        )
        (1): TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=512, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=512, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=512, out_features=256, bias=True)
            (k_proj): Linear(in_features=512, out_features=256, bias=True)
            (v_proj): Linear(in_features=512, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=512, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=512, out_features=256, bias=True)
        (k_proj): Linear(in_features=512, out_features=256, bias=True)
        (v_proj): Linear(in_features=512, out_features=256, bias=True)
        (out_proj): Linear(in_features=256, out_features=512, bias=True)
      )
      (norm_final_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (add_token): Embedding(1, 512)
    (output_upscaling): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LayerNorm2d()
      (2): GELU()
      (3): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU()
      (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
      (7): GELU()
    )
    (output_hypernetworks_mlp): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=512, bias=True)
        (2): Linear(in_features=512, out_features=64, bias=True)
      )
    )
    (pe_layer): PositionEmbeddingRandom()
  )
  (exo_cls): Sequential(
    (0): Linear(in_features=512, out_features=256, bias=True)
    (1): GELU()
    (2): Linear(in_features=256, out_features=36, bias=True)
  )
  (noun_transform): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (reason): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (context_aware): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (active_transform): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (passive_transform): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU()
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
)
Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.95]
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 0.01

Parameter Group 1
    amsgrad: False
    betas: [0.9, 0.95]
    eps: 1e-08
    initial_lr: 1e-05
    lr: 1e-05
    weight_decay: 0.01
)
============Training Epoch 0============
Training loss: 3.2217519879341125, KL loss: 2.2396899938583372, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.8956120699644089, Part sim loss: 0.7701460391283035 
Context_simm_loss: 0.9435327112674713
learning rate:0.0001

Result on AGD: 
KLD=2.243072509765625, SIM=0.224856436252594, NSS=1.5460408926010132
noun sim: 0.7569502234458924, part sim: 0.544544804096222
New best KLD: 2.243072509765625, 0.224856436252594, 1.5460408926010132
============Training Epoch 1============
Training loss: 2.274474549293518, KL loss: 1.6113114774227142, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.6135312378406524, Part sim loss: 0.4227611243724823 
Context_simm_loss: 0.7355676710605621
learning rate:0.0001

Result on AGD: 
KLD=1.905871868133545, SIM=0.28941839933395386, NSS=2.0979909896850586
noun sim: 0.5565670847892761, part sim: 0.2950437098741531
New best KLD: 1.905871868133545, 0.28941839933395386, 2.0979909896850586
============Training Epoch 2============
Training loss: 1.6956704080104827, KL loss: 1.2259250670671462, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.44144375175237655, Part sim loss: 0.23176355585455893 
Context_simm_loss: 0.5125249549746513
learning rate:0.0001

Result on AGD: 
KLD=1.874699354171753, SIM=0.26824450492858887, NSS=2.4393198490142822
noun sim: 0.3934356033802032, part sim: 0.15884280502796172
New best KLD: 1.874699354171753, 0.26824450492858887, 2.4393198490142822
============Training Epoch 3============
Training loss: 1.3219076693058014, KL loss: 1.0047684341669083, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.30081419795751574, Part sim loss: 0.13428480215370656 
Context_simm_loss: 0.28965504467487335
learning rate:0.0001

Result on AGD: 
KLD=1.6960369348526, SIM=0.29911109805107117, NSS=2.643435001373291
noun sim: 0.28468576073646545, part sim: 0.10584806352853775
New best KLD: 1.6960369348526, 0.29911109805107117, 2.643435001373291
============Training Epoch 4============
Training loss: 1.093844112753868, KL loss: 0.8622517138719559, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.21975381448864936, Part sim loss: 0.1045058611780405 
Context_simm_loss: 0.1388008665293455
learning rate:0.0001

Result on AGD: 
KLD=1.7405112981796265, SIM=0.30653128027915955, NSS=2.5302867889404297
noun sim: 0.22087366580963136, part sim: 0.09142767116427422
============Training Epoch 5============
Training loss: 0.8960503518581391, KL loss: 0.7039615452289582, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.18179039657115936, Part sim loss: 0.09430227354168892 
Context_simm_loss: 0.0868182934820652
learning rate:0.0001

Result on AGD: 
KLD=1.7754740715026855, SIM=0.285173237323761, NSS=2.4478540420532227
noun sim: 0.18333108723163605, part sim: 0.08401215225458145
============Training Epoch 6============
Training loss: 0.8228754431009293, KL loss: 0.6567078024148941, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.15659043863415717, Part sim loss: 0.08842269852757453 
Context_simm_loss: 0.0734937161207199
learning rate:0.0001

Result on AGD: 
KLD=1.7241958379745483, SIM=0.31033509969711304, NSS=2.5091702938079834
noun sim: 0.1602063685655594, part sim: 0.07931993305683135
============Training Epoch 7============
Training loss: 0.7026545464992523, KL loss: 0.5496244266629219, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.14384493939578533, Part sim loss: 0.08511781133711338 
Context_simm_loss: 0.06733941286802292
learning rate:0.0001

Result on AGD: 
KLD=1.8095057010650635, SIM=0.3235187530517578, NSS=2.2860989570617676
noun sim: 0.14702987670898438, part sim: 0.07687081247568131
============Training Epoch 8============
Training loss: 0.6501453846693039, KL loss: 0.5060854583978653, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.13498139157891273, Part sim loss: 0.0843419961631298 
Context_simm_loss: 0.06443451885133981
learning rate:0.0001

Result on AGD: 
KLD=1.7222957611083984, SIM=0.3311428725719452, NSS=2.438089370727539
noun sim: 0.13764030933380128, part sim: 0.07703593373298645
============Training Epoch 9============
Training loss: 0.6103064969182015, KL loss: 0.47290280759334563, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.1284782536327839, Part sim loss: 0.08297224752604962 
Context_simm_loss: 0.06282095201313495
learning rate:0.0001

Result on AGD: 
KLD=1.7763690948486328, SIM=0.3259911835193634, NSS=2.3789169788360596
noun sim: 0.1300707131624222, part sim: 0.07595914155244828
============Training Epoch 10============
Training loss: 0.5884903609752655, KL loss: 0.45564191341400145, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.12403078638017177, Part sim loss: 0.08198696002364159 
Context_simm_loss: 0.06189690995961428
learning rate:0.0001

Result on AGD: 
KLD=1.783743977546692, SIM=0.30328673124313354, NSS=2.4158875942230225
noun sim: 0.1255158856511116, part sim: 0.07587355226278306
============Training Epoch 11============
Training loss: 0.5396208763122559, KL loss: 0.4093842744827271, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.12132567763328553, Part sim loss: 0.08295288309454918 
Context_simm_loss: 0.06156285982578993
learning rate:0.0001

Result on AGD: 
KLD=1.6361591815948486, SIM=0.31710925698280334, NSS=2.7630045413970947
noun sim: 0.1235227793455124, part sim: 0.07506446763873101
New best KLD: 1.6361591815948486, 0.31710925698280334, 2.7630045413970947
============Training Epoch 12============
Training loss: 0.49544711261987684, KL loss: 0.36724305599927903, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11935782842338086, Part sim loss: 0.08234405145049095 
Context_simm_loss: 0.06118191219866276
learning rate:0.0001

Result on AGD: 
KLD=1.7171841859817505, SIM=0.3292771577835083, NSS=2.4639718532562256
noun sim: 0.11996186524629593, part sim: 0.07430987730622292
============Training Epoch 13============
Training loss: 0.46938419044017793, KL loss: 0.34383499920368193, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11688361428678036, Part sim loss: 0.08061367683112622 
Context_simm_loss: 0.060420841351151465
learning rate:0.0001

Result on AGD: 
KLD=1.7168784141540527, SIM=0.33176231384277344, NSS=2.5415725708007812
noun sim: 0.11642652750015259, part sim: 0.07395231500267982
============Training Epoch 14============
Training loss: 0.4704292342066765, KL loss: 0.3470358446240425, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11469845920801162, Part sim loss: 0.08092399649322032 
Context_simm_loss: 0.0602530250325799
learning rate:0.0001

Result on AGD: 
KLD=1.7534620761871338, SIM=0.3366733491420746, NSS=2.4333150386810303
noun sim: 0.11321627199649811, part sim: 0.07329458072781563
============Training Epoch 15============
Training loss: 0.42553887218236924, KL loss: 0.3041353337466717, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11271818652749062, Part sim loss: 0.08085202090442181 
Context_simm_loss: 0.06001457329839468
learning rate:0.0001

Result on AGD: 
KLD=1.9543426036834717, SIM=0.30855247378349304, NSS=2.117626190185547
noun sim: 0.11109428107738495, part sim: 0.07288630306720734
============Training Epoch 16============
Training loss: 0.42772242724895476, KL loss: 0.30765621215105055, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.11147444732487202, Part sim loss: 0.07994479574263096 
Context_simm_loss: 0.05972926542162895
learning rate:0.0001

Result on AGD: 
KLD=1.9165462255477905, SIM=0.32653436064720154, NSS=2.1362743377685547
noun sim: 0.10992293953895568, part sim: 0.07338985949754714
============Training Epoch 17============
Training loss: 0.40865811705589294, KL loss: 0.2895495221018791, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.1104352381080389, Part sim loss: 0.08076357878744603 
Context_simm_loss: 0.05970028601586819
learning rate:0.0001

Result on AGD: 
KLD=1.7755593061447144, SIM=0.31910762190818787, NSS=2.547398805618286
noun sim: 0.10855648666620255, part sim: 0.0727258212864399
============Training Epoch 18============
Training loss: 0.4001464143395424, KL loss: 0.2816804893314838, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10983403250575066, Part sim loss: 0.08027575388550759 
Context_simm_loss: 0.06043190285563469
learning rate:0.0001

Result on AGD: 
KLD=1.6874550580978394, SIM=0.34465670585632324, NSS=2.6069579124450684
noun sim: 0.10765942037105561, part sim: 0.07215779423713684
============Training Epoch 19============
Training loss: 0.3822503760457039, KL loss: 0.26454486772418023, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10913521461188794, Part sim loss: 0.07972813062369824 
Context_simm_loss: 0.05974860806018114
learning rate:5e-05

Result on AGD: 
KLD=2.049377679824829, SIM=0.33325812220573425, NSS=2.0517797470092773
noun sim: 0.10585110634565353, part sim: 0.07210332602262497
============Training Epoch 20============
Training loss: 0.36753498017787933, KL loss: 0.2514814741909504, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10746905989944935, Part sim loss: 0.0798836525529623 
Context_simm_loss: 0.05960786435753107
learning rate:5e-05

Result on AGD: 
KLD=1.7202162742614746, SIM=0.3433452844619751, NSS=2.607232093811035
noun sim: 0.10530031621456146, part sim: 0.07175438553094864
============Training Epoch 21============
Training loss: 0.3400009453296661, KL loss: 0.22418836876749992, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10727681629359723, Part sim loss: 0.07942131422460079 
Context_simm_loss: 0.05936308279633522
learning rate:5e-05

Result on AGD: 
KLD=1.8417242765426636, SIM=0.326080322265625, NSS=2.2936532497406006
noun sim: 0.10569261908531188, part sim: 0.07175111398100853
============Training Epoch 22============
Training loss: 0.3373423159122467, KL loss: 0.2215370260179043, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10729600563645363, Part sim loss: 0.0790965186432004 
Context_simm_loss: 0.05996404550969601
learning rate:5e-05

Result on AGD: 
KLD=1.7823232412338257, SIM=0.31436020135879517, NSS=2.458465814590454
noun sim: 0.1047466054558754, part sim: 0.07154237478971481
============Training Epoch 23============
Training loss: 0.3156405113637447, KL loss: 0.20043528005480765, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10670378059148788, Part sim loss: 0.07912182286381722 
Context_simm_loss: 0.05892648063600063
learning rate:5e-05

Result on AGD: 
KLD=1.8514084815979004, SIM=0.3249787986278534, NSS=2.330777883529663
noun sim: 0.10459374189376831, part sim: 0.07137258797883987
============Training Epoch 24============
Training loss: 0.305630262196064, KL loss: 0.19053323492407798, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10660302266478539, Part sim loss: 0.07901287898421287 
Context_simm_loss: 0.05927177015691996
learning rate:5e-05

Result on AGD: 
KLD=1.8162741661071777, SIM=0.3147094249725342, NSS=2.391256093978882
noun sim: 0.10414160192012786, part sim: 0.07103532180190086
============Training Epoch 25============
Training loss: 0.3056815505027771, KL loss: 0.19088462740182877, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10630478709936142, Part sim loss: 0.07900361754000187 
Context_simm_loss: 0.05917726494371891
learning rate:5e-05

Result on AGD: 
KLD=1.909302830696106, SIM=0.3344142735004425, NSS=2.2524986267089844
noun sim: 0.10371581614017486, part sim: 0.07088002189993858
============Training Epoch 26============
Training loss: 0.29458569586277006, KL loss: 0.18006204925477504, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10603827275335789, Part sim loss: 0.07890662290155888 
Context_simm_loss: 0.0594711834564805
learning rate:5e-05

Result on AGD: 
KLD=1.75070321559906, SIM=0.33207985758781433, NSS=2.4936277866363525
noun sim: 0.10383699983358383, part sim: 0.07094526439905166
============Training Epoch 27============
Training loss: 0.2908895552158356, KL loss: 0.17650933787226677, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10600479580461979, Part sim loss: 0.07784362249076367 
Context_simm_loss: 0.059105978906154634
learning rate:5e-05

Result on AGD: 
KLD=1.824145793914795, SIM=0.3287573754787445, NSS=2.343594789505005
noun sim: 0.1028036504983902, part sim: 0.07102971076965332
============Training Epoch 28============
Training loss: 0.28632906451821327, KL loss: 0.17228833287954332, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10555134080350399, Part sim loss: 0.07894576080143452 
Context_simm_loss: 0.05948184691369533
learning rate:5e-05

Result on AGD: 
KLD=1.8226605653762817, SIM=0.32209715247154236, NSS=2.3959176540374756
noun sim: 0.1026758924126625, part sim: 0.07089263573288918
============Training Epoch 29============
Training loss: 0.3070988766849041, KL loss: 0.1935093566775322, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10510774478316307, Part sim loss: 0.07889866456389427 
Context_simm_loss: 0.0591913178563118
learning rate:5e-05

Result on AGD: 
KLD=1.8182207345962524, SIM=0.329561322927475, NSS=2.364438056945801
noun sim: 0.10239671021699906, part sim: 0.07118551135063171
============Training Epoch 30============
Training loss: 0.287661737203598, KL loss: 0.17428873293101788, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10492707751691341, Part sim loss: 0.07853635549545288 
Context_simm_loss: 0.059229314886033536
learning rate:5e-05

Result on AGD: 
KLD=1.8419584035873413, SIM=0.3293967843055725, NSS=2.3206522464752197
noun sim: 0.10204003006219864, part sim: 0.07093067765235901
============Training Epoch 31============
Training loss: 0.27411847561597824, KL loss: 0.1606410700827837, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10496193356812, Part sim loss: 0.07920889370143414 
Context_simm_loss: 0.059457927383482455
learning rate:5e-05

Result on AGD: 
KLD=1.862371563911438, SIM=0.307657927274704, NSS=2.301142454147339
noun sim: 0.10232417583465576, part sim: 0.07092182338237762
============Training Epoch 32============
Training loss: 0.27597485929727555, KL loss: 0.1626704927533865, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10489432029426098, Part sim loss: 0.0781826589256525 
Context_simm_loss: 0.059177924506366254
learning rate:5e-05

Result on AGD: 
KLD=1.8600776195526123, SIM=0.34597596526145935, NSS=2.3495774269104004
noun sim: 0.10190792679786682, part sim: 0.07105251327157021
============Training Epoch 33============
Training loss: 0.26718336418271066, KL loss: 0.15405610650777818, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10474353395402432, Part sim loss: 0.07795533798635006 
Context_simm_loss: 0.05881867203861475
learning rate:5e-05

Result on AGD: 
KLD=1.8419617414474487, SIM=0.31641411781311035, NSS=2.332348108291626
noun sim: 0.1018472358584404, part sim: 0.07087274491786957
============Training Epoch 34============
Training loss: 0.2647163398563862, KL loss: 0.1518121764063835, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10443974137306214, Part sim loss: 0.07871862053871155 
Context_simm_loss: 0.05925593674182892
learning rate:5e-05

Result on AGD: 
KLD=1.7588605880737305, SIM=0.342096209526062, NSS=2.5293054580688477
noun sim: 0.10156813859939576, part sim: 0.07070275992155076
============Training Epoch 35============
Training loss: 0.27429247125983236, KL loss: 0.16142861172556877, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10439162962138653, Part sim loss: 0.07876659780740738 
Context_simm_loss: 0.05955698043107986
learning rate:5e-05

Result on AGD: 
KLD=1.914048194885254, SIM=0.3359468877315521, NSS=2.2264404296875
noun sim: 0.10126603692770005, part sim: 0.07057904675602913
============Training Epoch 36============
Training loss: 0.26147371158003807, KL loss: 0.14858749657869338, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.104447278752923, Part sim loss: 0.07849718853831292 
Context_simm_loss: 0.05892149899154901
learning rate:5e-05

Result on AGD: 
KLD=1.8393200635910034, SIM=0.32216939330101013, NSS=2.3827524185180664
noun sim: 0.10162191987037658, part sim: 0.07073514610528946
============Training Epoch 37============
Training loss: 0.2560653299093246, KL loss: 0.14357722625136377, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10410996712744236, Part sim loss: 0.07790904827415943 
Context_simm_loss: 0.058722897060215475
learning rate:5e-05

Result on AGD: 
KLD=1.8588263988494873, SIM=0.3343333899974823, NSS=2.4160068035125732
noun sim: 0.10103300660848617, part sim: 0.07044321596622467
============Training Epoch 38============
Training loss: 0.2512861959636211, KL loss: 0.13907207548618317, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10386235937476158, Part sim loss: 0.07762144226580858 
Context_simm_loss: 0.0589618606492877
learning rate:5e-05

Result on AGD: 
KLD=1.875257134437561, SIM=0.3301053047180176, NSS=2.325141668319702
noun sim: 0.10088493525981904, part sim: 0.07056985571980476
============Training Epoch 39============
Training loss: 0.2508408509194851, KL loss: 0.13861249051988125, Sim loss: 0.0, Exo CLS loss: 0.0, 
Noun sim loss: 0.10384552106261254, Part sim loss: 0.07793803215026855 
Context_simm_loss: 0.05890376511961222
learning rate:2.5e-05

Result on AGD: 
KLD=1.8801686763763428, SIM=0.3327730596065521, NSS=2.394948720932007
noun sim: 0.10089675635099411, part sim: 0.07062819451093674
